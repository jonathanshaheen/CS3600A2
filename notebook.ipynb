{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If you are reading this notebook on the GitHub, please go to [README](./README.md) and follow installation instructions to set everything up locally, it's an interactive notebook and you need a local setup to execute the cells. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Assignment 1 - Swap Isolation!\n",
    "\n",
    "Your task is to create an AI that can play and win a game of Swap Isolation. Your AI will be tested against several pre-baked AIs as well as your peers’ AI systems. You will implement your AI in Python 3.7, using our provided code as a starting point. \n",
    "\n",
    "In case we haven't got this into your heads enough: **start early!!!** It is entirely possible, and probably likely, that a large part of your next 2 weeks will be devoted to this assignment, but we hope that it is worth it and you will really enjoy the assignment! \n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Table of contents\n",
    "\n",
    "0. [About the Game](#About-the-Game)\n",
    "1. [Important Files](#Important-Files)\n",
    "2. [The Assignment](#The-Assignment)\n",
    "3. [Submissions & Grading](#Submissions-&-Grading)\n",
    "4. [Exporting the notebook](#Exporting-the-notebook)\n",
    "5. [Coding-time!](#Coding-time!)\n",
    "6. [Section1a checkpoint!]()\n",
    "7. [Section1b checkpoint!]()\n",
    "8. [Section1c checkpoint!]()\n",
    "9. [Bot fight!](#Botfight-(Extra-Credit))\n",
    "\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules of Swap Isolation are simple. There are two players, each with his own game piece, and a 7-by-7 grid of squares. At the beginning of the game, the first player places his piece on any square. The second player follows suit, and places his piece on any one of the available squares. From that point on, the players alternate turns moving their piece like a queen in chess (any number of open squares vertically, horizontally, or diagonally). When the piece is moved, the square that was previously occupied is blocked, and can not be used for the remainder of the game.\n",
    "\n",
    "A queen can not move through blocked squares. She can, however, move into a position occupied by another queen, in which case the other queen now occupies the position that was held by this queen. This is called a 'swap'. You cannot swap back with the other queen if a swap involving both queen's positions already happened the previous turn. To make this clearer, let's use an example. If Q1 performed a swap with Q2 during Turn 1, where Q1 was in spot A and Q2 in spot B, then on Turn 2, Q2 **cannot** do a swap to take back spot B. #TODO MUKUND specify any distance swaps\n",
    "\n",
    "The first player who is unable to move their queen loses.\n",
    "\n",
    "You can try playing the game against the Random Player or Yourself below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%run helpers/verify_config.py # verify the environment setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following two lines make sure anything imported from .py scripts \n",
    "# is automatically reloaded if edited & saved (e.g. local unit tests or players)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from board_viz import ReplayGame, InteractiveGame\n",
    "from isolation import Board\n",
    "from test_players import RandomPlayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace RandomPlayer() with None if you want to play for both players\n",
    "ig = InteractiveGame(RandomPlayer(), show_legal_moves=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One other thing you can do is simulate a game between two players and replay it.\n",
    "\n",
    "**Run next cell, place your cursor inside text input box right above the slider and press Up or Down.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is an example of how to visualise a game replay of 2 random players\n",
    "game = Board(RandomPlayer(), RandomPlayer(), 7, 7)\n",
    "winner, move_history, termination = game.play_isolation(time_limit=1000, print_moves=False)\n",
    "\n",
    "bg = ReplayGame(game, move_history, show_legal_moves=True)\n",
    "bg.show_board()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important Files\n",
    "While you'll only have to edit `notebook.ipynb` and submit exported `submission.py`, there are a number of notable files:\n",
    "\n",
    "1. `isolation.py`: Includes the `Board` class and a function for printing out a game as text. Do **NOT** change contents of this file. We have the same file on the server's side, so any changes will not be accounted for.\n",
    "2. `notebook.ipynb`: Where you'll implement the required methods for your agents.\n",
    "3. `player_submission_tests.py`: Sample tests to validate your agents locally.\n",
    "4. `test_players.py`: Containts 2 player types for you to test agents locally:\n",
    "    - `RandomPlayer` - chooses a legal move randomly from among the available legal moves\n",
    "    - `HumanPlayer` - allows *YOU* to play against the AI\n",
    "\n",
    "Additionally, we've included a number of local unittests to help you test your player and evaluation function as well as to give you an idea of how the classes are used. Feel free to play around with them and add more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Assignment\n",
    "\n",
    "In this assignment you will need to implement evaluation functions and game playing methods. Your goal is to implement the following parts of the notebook:\n",
    "\n",
    "1. Evaluation functions (`OpenMoveEvalFn` and `CustomEvalFn`)\n",
    "2. The minimax algorithm (`minimax`)\n",
    "3. Alpha-beta pruning (`alphabeta`)\n",
    "4. Adjust the `move()` according to section you are trying to work on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions\n",
    "\n",
    "These functions will inform the value judgements your AI will make when choosing moves. There are 2 classes:\n",
    "\n",
    "- `OpenMoveEvalFn` -Returns the number of available moves open for your player minus the number of moves available for opponent player. All baseline tests will use this function. **This is mandatory**\n",
    "- `CustomEvalFn` - You are encouraged to create your own evaluation function here.\n",
    "\n",
    "**DO** submit the code within this class (and only the code within this class).\n",
    "\n",
    "#### Notes on these classes\n",
    "1. You may write additional code within each class. However, we will only be invoking the `score()` function. You may not change the signature of this function.\n",
    "2. When writing additional code to test, try to do so in separate classes (do not use ours). It allows for independent test execution and you can be sure that *all* the code within the EvalFn cells belong only to the EvalFn classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomPlayer\n",
    "\n",
    "This is the meat of the assignment. A few notes about the class:\n",
    "\n",
    "- You are not permitted to change the function signatures of any of the provided methods.\n",
    "- You are permitted to change the default values within the function signatures provided. In fact, when you have your custom evaluation function, you are encouraged to change the default values for `__init__` to use the new eval function.\n",
    "- You are free change the contents of each of the provided methods. When you are ready with `alphabeta()`, for example, you are encouraged to update `move()` to use that function instead.\n",
    "- You are free to add more methods to the class.\n",
    "- You may not create additional external functions and classes that are referenced from within this class.\n",
    "\n",
    "**DO** submit the code within this class (and only the code within this class).\n",
    "\n",
    "Your agent will have a limited amount of time to act each turn (1 second). We will call these functions directly so **don’t modify** the <u>function names</u> or the <u>parameters</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to checking time each turn, you will be penalized if your agent takes more than a few minutes at construction time (for example, if you attempt to load the entire set of possible board states from memory). We have divided the tests into three(mentioned in details in next grading section below).  In total, your submission will be allowed to run for a maximum of <u>30 minutes</u> before being interrupted for the first section. This is increased to <u>120 minutes</u> for the second and third section.\n",
    "\n",
    "These are the bare minimum requirements for your AI, and the rest is up to you. You will be scored according to how well your AI performs against some baseline AIs that we provide (see “Grading”). If you want to improve over the base performance, here are a few suggestions:\n",
    "\n",
    "- Use partition techniques.\n",
    "- Store the evaluation scores for past moves.\n",
    "- Modify your evaluation function to account for “killer moves” and \"pushes\".\n",
    "- Optimize functions that are called often.\n",
    "- Order nodes to maximize pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submissions & Grading\n",
    "\n",
    "The grade you receive for the assignment will be determined as follows:\n",
    "\n",
    "|Section| Points    | Condition                                |\n",
    "| :---------| :--------- | :---------------------------------------- |\n",
    "|1a| 5 points | You write an evaluation function, OpenMoveEval, which returns the number of moves that the AI minus the number of moves opponent can make, and your evaluation function performs correctly on some sample boards we provide. |\n",
    "|1a| 30 points | Your AI defeats a random player >= 90% of the time. |\n",
    "|1b| 20 points | Your AI defeats an agent with OpenMoveEval function that uses minimax to level 2  >= 65% of the times. |\n",
    "|1b| 20 points | Your AI defeats an agent with OpenMoveEval function that uses alphabeta to level 4  >= 65% of the times. |\n",
    "|1c| 20 points | Your AI defeats an agent with OpenMoveEval function that uses iterative deepening and alpha-beta pruning >= 65% of the time. |\n",
    "|1c| 5 points | Your AI defeats an agent with Peter's secret evaluation function that uses iterative deepening and alpha-beta pruning and optimizes various aspects of the game player >= 85% of the time  |\n",
    "\n",
    "As you can see from the table there are three autograded sections, each having the following submission frequency restrictions:\n",
    "- **Section 1a** - 1 submission per 30 minutes.\n",
    "- **Section 1b** - 1 submission per 120 minutes.\n",
    "- **Section 1c** - 1 submission per 120 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will provide you checkpoints and instructions below once you are ready to submit for each of these sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exporting the notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do get your submission file raedy you will need to make sure have **Saved your notebook** and run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run helpers/notebook2script section1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once execution is complete open autogenerated `submission.py` and verify that it contains all of the imports, functions and classes you are required to implement. Only then you can proceed to the Gradescope for submission.\n",
    "\n",
    "**Do NOT erase the `#export` at the top of any cells it's is used by `notebook2script.py` to extract the bits for submission.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing External Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following two lines make sure anything imported from .py scripts \n",
    "# is automatically reloaded if edited & saved (e.g. local unit tests or players)\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import player_submission_tests as tests\n",
    "from test_players import HumanPlayer, RandomPlayer\n",
    "from board_vis import BoardGrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import time\n",
    "from isolation import Board"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have discussed this assignment at a whiteboard level, got help from piazza or have use external resources (not provided) that you may want to cite, please do so in the cell as a python comment! (no need to cite python or included packages documentation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Credits if any\n",
    "# 1)\n",
    "# 2)\n",
    "# 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenMoveEvalFn\n",
    "- This is where you write your evaluation function to evaluate the state of the board.\n",
    "- The test cases below the code is expected to pass locally.\n",
    "- Hints: This needs to work for when either your agent or the opponent is making a move. In other words, perspective is important. #TODO talk to Peter\n",
    "\n",
    "Here are the few method you might find useful to implement `OpenMoveEvalFn()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Board.get_player_moves?? #TODO update with new version - PETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Board.get_opponent_moves??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class OpenMoveEvalFn:\n",
    "    def score(self, game, my_turn=True):\n",
    "        \"\"\"Score the current game state\n",
    "\n",
    "        Evaluation function that outputs a score equal to how many\n",
    "        moves are open for AI player on the board minus how many moves\n",
    "        are open for Opponent's player on the board.\n",
    "        Note:\n",
    "            1. Be very careful while doing opponent's moves. You might end up\n",
    "               reducing your own moves.\n",
    "            3. If you think of better evaluation function, do it in CustomEvalFn below.\n",
    "\n",
    "            Args\n",
    "                param1 (Board): The board and game state.\n",
    "                param2 (bool): True if maximizing player is active.\n",
    "\n",
    "            Returns:\n",
    "                float: The current state's score. MyMoves-OppMoves.\n",
    "\n",
    "            \"\"\"\n",
    "\n",
    "        # TODO: finish this function!\n",
    "        raise NotImplementedError\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.correctOpenEvalFn(OpenMoveEvalFn)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the local test above\n",
    "If you want to edit the test (which you most definitely can), then edit the source code back in `player_submission_tests.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomPlayer\n",
    "- CustomPlayer is the player object that will be used to play the game of isolation.\n",
    "- The `move()` method will be used to pass you over the current state of the game board.\n",
    "- The content of the `move()` method will be changed by you according to the section you are attempting to pass. While you can use Iterative Deepening & Alpha-Beta (ID+AB) to beat our agents in all of the sections, going directly for ID+AB is error prone, we highly recommend you to start with MiniMax (MM), then implement Alpha-Beta (AB) and only then go for ID+AB.\n",
    "- By default, right now it calls `minimax()` as you can see below.\n",
    "- You are not allowed to modify the function or class signatures we provide, however, in case you want to have an additonal parameter you can do it at the very end, see example below. However, it must have a default value and you shouldn't expect it be passed on the server-size (i.e. Gradescope), hence, the default value will be used.\n",
    "\n",
    "Originally:\n",
    "```python\n",
    "def move(self, game, legal_moves, time_left):\n",
    "    ...\n",
    "```\n",
    "Adding a new argument with default parameter.\n",
    "```python\n",
    "def move(self, game, legal_moves, time_left, new_parameter=default_value):\n",
    "    ...\n",
    "```\n",
    "\n",
    "Don't do this, you will get an error on autograded and lose your submission:\n",
    "```python\n",
    "def move(self, game, legal_moves, time_left, new_parameter):\n",
    "    ...\n",
    "```\n",
    "```python\n",
    "def move(self, new_parameter, game, legal_moves, time_left):\n",
    "    ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For **Section 1a** you can simply run the next cell and continue to `minimax()` implementation #TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CustomPlayer:\n",
    "    # TODO: finish this class!\n",
    "    \"\"\"Player that chooses a move using your evaluation function\n",
    "    and a minimax algorithm with alpha-beta pruning.\n",
    "    You must finish and test this player to make sure it properly\n",
    "    uses minimax and alpha-beta to return a good move.\"\"\"\n",
    "\n",
    "    def __init__(self, search_depth=3, eval_fn=OpenMoveEvalFn()):\n",
    "        \"\"\"Initializes your player.\n",
    "        \n",
    "        if you find yourself with a superior eval function, update the default\n",
    "        value of `eval_fn` to `CustomEvalFn()`\n",
    "        \n",
    "        Args:\n",
    "            search_depth (int): The depth to which your agent will search\n",
    "            eval_fn (function): Utility function used by your agent\n",
    "        \"\"\"\n",
    "        self.eval_fn = eval_fn\n",
    "        self.search_depth = search_depth\n",
    "    \n",
    "    def move(self, game, legal_moves, time_left):\n",
    "        \"\"\"Called to determine one move by your agent\n",
    "\n",
    "        Note:\n",
    "            1. Do NOT change the name of this 'move' function. We are going to call\n",
    "            the this function directly.\n",
    "            2. Change the name of minimax function to alphabeta function when\n",
    "            required. Here we are talking about 'minimax' function call,\n",
    "            NOT 'move' function name.\n",
    "            Args:\n",
    "            game (Board): The board and game state.\n",
    "            legal_moves (list): List of legal moves\n",
    "            time_left (function): Used to determine time left before timeout\n",
    "\n",
    "        Returns:\n",
    "            tuple: best_move\n",
    "        \"\"\"\n",
    "        best_move, utility = minimax(self, game, time_left, depth=self.search_depth)\n",
    "        return best_move\n",
    "\n",
    "    def utility(self, game):\n",
    "        \"\"\"Can be updated if desired. Not compulsory.\"\"\"\n",
    "        return self.eval_fn.score(game, self)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minimax\n",
    "- This is where you will implement the minimax algorithm. The final output of your minimax should come from this method and this is the only method that Gradescope will call when testing minimax.\n",
    "- Test cases the below code is expected to pass: The first 30 points (as specified in the README).\n",
    "- Useful functions: The useful methods will probably all come from isolation.py. A couple of particularly interesting ones could be forecast_move() and your score() method from OpenMoveEvalFn. Remember the double question mark trick from Assignment 0 if you feel you are flipping between files too much!\n",
    "- Hints: Remember that perspective hint from before? If you did not realize what it meant and made a slight error there, you may catch it here. #TODO MUKUND/PETER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def minimax(player, game, time_left, depth, my_turn=True):\n",
    "    \"\"\"Implementation of the minimax algorithm.\n",
    "    \n",
    "    Args:\n",
    "        #TODO player???\n",
    "        #TODO player, mostly used for player.utility and anythinge else inside the Player class\n",
    "        game (Board): A board and game state.\n",
    "        time_left (function): Used to determine time left before timeout\n",
    "        depth: Used to track how deep you are in the search tree\n",
    "        maximizing_player (bool): True if maximizing player is active. #TODO my_turn!!!\n",
    "        \n",
    "    Returns:\n",
    "        (tuple, int): best_move, val\n",
    "    \"\"\"\n",
    "    # TODO: finish this function!\n",
    "    raise NotImplementedError\n",
    "    return best_move, best_val\n",
    "\n",
    "#TODO Check with working code\n",
    "\n",
    "########## DON'T WRITE ANY CODE OUTSIDE THE FUNCTION! ################\n",
    "##### CODE BELOW IS USED FOR RUNNING LOCAL TEST DON'T MODIFY IT ######\n",
    "tests.beatRandom(CustomPlayer)\n",
    "tests.minimaxTest(minimax)\n",
    "################ END OF LOCAL TEST CODE SECTION ######################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Now it's a good time to submit for Section1a - See (Exporting the notebook)[#Exporting-the-notebook]\n",
    "\n",
    "In case you want to submit please uncomment and run the cell below.\n",
    "\n",
    "Your code will be generated in the folder named `section1a`, please upload `submission.py` file to [Gradescope](https://www.gradescope.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run helpers/notebook2script section1a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlphaBeta\n",
    "- This is where you will implement the alphabeta algorithm. The final output of your alphabeta should come from this method and this is the only method that Gradescope will call when testing alphabeta.\n",
    "- Test cases the below code is expected to pass: Minimax level 2 >= 65% of the time\n",
    "- Useful functions: The useful methods will probably all come from isolation.py. A couple of particularly interesting ones could be forecast_move() and your score() method from OpenMoveEvalFn. Remember the double question mark trick from Assignment 0 if you feel you are flipping between files too much!\n",
    "- Hints: Remember a key point of alphabeta: your final answer will always be the same as if you implemented minimax, but you will get that answer a lot faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def alphabeta(player, game, time_left, depth, alpha=float(\"-inf\"), beta=float(\"inf\"), my_turn=True):\n",
    "    \"\"\"Implementation of the alphabeta algorithm.\n",
    "    \n",
    "    Args:\n",
    "        game (Board): A board and game state.\n",
    "        time_left (function): Used to determine time left before timeout\n",
    "        depth: Used to track how deep you are in the search tree\n",
    "        alpha (float): Alpha value for pruning\n",
    "        beta (float): Beta value for pruning\n",
    "        maximizing_player (bool): True if maximizing player is active.\n",
    "        \n",
    "    Returns:\n",
    "        (tuple, int): best_move, val\n",
    "    \"\"\"\n",
    "    # TODO: finish this function!\n",
    "    raise NotImplementedError\n",
    "    return best_move, val\n",
    "\n",
    "# ADD YOU LOCAL TESTS HERE IF DESIRED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#TODO reminder to change the `move()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the lack of a local test above\n",
    "Notice that we do not have any code here. We want you to learn to write your own test cases, so feel free to get creative! You can always create the test in player_submission_tests.py and then run it over here in a manner identical to how local tests have been run so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Now it's a good time to submit for Section1b - See (Exporting the notebook)[#Exporting-the-notebook]\n",
    "\n",
    "In case you want to submit please uncomment and run the cell below.\n",
    "\n",
    "Your code will be generated in the folder named `section1b`, please upload `submission.py` file to [Gradescope](https://www.gradescope.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run helpers/notebook2script section1b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## That does not cover all 100 points though!\n",
    "- You're right, and that's on purpose. Each of the bullets below try to walk you through how you may want to think about beating the remaining agents.\n",
    "    - First up is the alphabeta agent. Vanilla alphabeta (that is, alphabeta with no optimization) may not do so well against this agent. However, any agent that searches deeper with the same algorithm probably has a bigger advantage. You may learn about a method that allows your algorithm to search in such a way that you can find the maximum search depth without running out of time. This will probably come up in class or you can read through the book to find out what you are looking for.\n",
    "    - Next is the agent with iterative deepening (yes, I know that I just gave away the answer). This one is a little harder to think about, given that you may have used all the tools that you may think of to try a make a \"better\" agent. But you may have just implemented the evaluation function that was discussed in class. Maybe we can do better - like checking for winning moves and prioritizing those! Or if you are feeling really creative, you can always try editing the \"CustomEvalFn\" at the bottom of this notebook and come up with an awesome idea of your own.\n",
    "    - Now to that agent with the evaluation function. I have nothing to say. Unfortunately, the TAs cannot say anything about what it will take to beat this agent and that is so that we can see what you all can come up with. Use everything in your toolbox and within the class rules to defeat it. And as always: good luck and have fun!\n",
    "    \n",
    "- Remember that you may want to edit the methods in the cell with the CustomPlayer class to try and implement some of the above. You are certainly free too as long as you don't change the function signatures (other than that little caveat that was mentioned in the cell above that code)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomEvalFn\n",
    "- Go crazy with how you want to design your own evaluation function. The typical rules about how you can and cannot edit the code we have given (namely, the function signature rules) apply here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class CustomEvalFn:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def score(self, game, maximizing_player_turn=True):\n",
    "        \"\"\"Score the current game state.\n",
    "        \n",
    "        Custom evaluation function that acts however you think it should. This\n",
    "        is not required but highly encouraged if you want to build the best\n",
    "        AI possible.\n",
    "        \n",
    "        Args:\n",
    "            game (Board): The board and game state.\n",
    "            maximizing_player_turn (bool): True if maximizing player is active.\n",
    "            \n",
    "        Returns:\n",
    "            float: The current state's score, based on your own heuristic.\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO: finish this function!\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO reminder to change the `move()` & CustomPlayer constructor (eval_fn())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Now it's a good time to submit for Section1c - See (Exporting the notebook)[#Exporting-the-notebook]\n",
    "\n",
    "In case you want to submit please uncomment and run the cell below.\n",
    "\n",
    "Your code will be generated in the folder named `section1c`, please upload `submission.py` file to [Gradescope](https://www.gradescope.com/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %run helpers/notebook2script section1c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Botfight (Extra Credit)\n",
    "\n",
    "In addition to the basic assignment, you will have the option to compete against your peers for the glory of being the Fall 2019 AI-Game-Playing champ. We’ll set up a system to pit your AI against others, and we’ll be handing out extra credit for the top players. May the odds be ever in your favor.\n",
    "\n",
    "If you compete in the AI tournament and your agent finishes in the top 10, you will receive bonus points for this assignment **(bonus points are added to the grades of each assignment. Not to final score. )**:\n",
    "\n",
    "- Best Overall:  12 bonus points added to the assignment score.\n",
    "- Second Best: 10 bonus points.\n",
    "- Third Best: 7 bonus points.\n",
    "- Fourth to Tenth Best: 5 bonus points.\n",
    "\n",
    "To make your submission simply upload a file called `submission.py` that contains `CustomPlayer()` class with your best agent implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contribute to the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you find any typos, have some issues or suggestions on how to improve this or any future assignments please fill free to create a Pull Request or make a Piazza post."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<!-- Hi there! -->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
