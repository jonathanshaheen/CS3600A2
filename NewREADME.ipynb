{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1 - Isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clone this repository:\n",
    "`git clone https://github.gatech.edu/omscs6601/assignment_1.git`\n",
    "\n",
    "The submission scripts depend on the presence of 4 python packages - `requests`, `future`, and `nelson`. Install them using the command below:\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "Read [setup.md](./setup.md) for more information on how to effectively manage your git repository and troubleshooting information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This assignment will cover some of the concepts discussed in the Adversarial Search lectures. You will be implementing game playing agents for a variant of the game Isolation.\n",
    "\n",
    "We are also implementing this through Jupyter Notebook, so you all may find it useful to spend some time getting familiar with this software. During the first week of classes, there was an assignment called \"Assignment 0\" that spends some time going through Python and Jupyter. If you are unfamiliar with either Python or Jupyter, please take some time to go through that assignment!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rules of Swap Isolation are simple. There are two players, each with his own game piece, and a 7-by-7 grid of squares. At the beginning of the game, the first player places his piece on any square. The second player follows suit, and places his piece on any one of the available squares. From that point on, the players alternate turns moving their piece like a queen in chess (any number of open squares vertically, horizontally, or diagonally). When the piece is moved, the square that was previously occupied is blocked, and can not be used for the remainder of the game.\n",
    "\n",
    "A queen can not move through blocked squares. She can, however, move into a position occupied by another queen, in which case the other queen now occupies the position that was held by this queen. This is called a 'swap'. You cannot swap back with the other queen if a swap involving both queen's positions already happened the previous turn. To make this clearer, let's use an example. If Q1 performed a swap with Q2 during Turn 1, where Q1 was in spot A and Q2 in spot B, then on Turn 2, Q2 **cannot** do a swap to take back spot B.\n",
    "\n",
    "The first player who is unable to move their queen loses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While you'll only have to edit and submit `player_submission.py`, there are a number of notable files:\n",
    "\n",
    "1. `isolation.py`: Includes the `Board` class and a function for printing out a game as text. Do NOT change contents of this file. We have the same file on the server's side, so any changes will not be accounted for.\n",
    "2. `player_submission.ipynb`: Where you'll implement the required methods for your agents.\n",
    "3. `player_submission_tests.py`: Sample tests to validate your agents locally.\n",
    "3. `test_players.py`: Example agents used to play isolation locally.\n",
    "4. `submit.py`: Script to submit your work to evaluate against the first 2 tests (mentioned in the next section).\n",
    "5. `submit_a.py`: Script to submit your work to evaluate against the middle 2 tests (mentioned in the next section).\n",
    "6. `submit_b.py`: Script to submit your work to evaluate against the last 2 tests (mentioned in the next section)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your task is to create an AI that can play and win a game of Sumo Isolation. Your AI will be tested against several pre-baked AIs as well as your peers’ AI systems. You will implement your AI in Python 2.7, using our provided code as a starting point.\n",
    "\n",
    "In this repository, we provide:\n",
    "\n",
    "- A class for representing the game state\n",
    "- A function for printing the game board\n",
    "- A function for generating legal game states\n",
    "- A class for running unit tests\n",
    "- A random AI (baseline test)\n",
    "\n",
    "Your goal is to implement the following parts of the AI in the class CustomPlayer:\n",
    "\n",
    "1. Evaluation functions (`OpenMoveEvalFn()` and `CustomEvalFn()`)\n",
    "2. The minimax algorithm (`minimax()`)\n",
    "3. Alpha-beta pruning (`alphabeta()`)\n",
    "\n",
    "Your agent will have a limited amount of time to act each turn (1 second). We will call these functions directly so **don’t modify** the <u>function names</u> or the <u>parameters</u>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to checking time each turn, you will be penalized if your agent takes more than a few minutes at construction time (for example, if you attempt to load the entire set of possible board states from memory). We have divided the tests into three(mentioned in details in next grading section below).  In total, your submission will be allowed to run for a maximum of <u>30 minutes</u> before being interrupted for the first section. This is increased to <u>120 minutes</u> for the second and third section.\n",
    "\n",
    "These are the bare minimum requirements for your AI, and the rest is up to you. You will be scored according to how well your AI performs against some baseline AIs that we provide (see “Grading”). If you want to improve over the base performance, here are a few suggestions:\n",
    "\n",
    "- Use partition techniques.\n",
    "- Store the evaluation scores for past moves.\n",
    "- Modify your evaluation function to account for “killer moves” and \"pushes\".\n",
    "- Optimize functions that are called often.\n",
    "- Order nodes to maximize pruning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grade you receive for the assignment will be determined as follows:\n",
    "\n",
    "| Points    | Condition                                |\n",
    "| --------- | ---------------------------------------- |\n",
    "| 5 points | You write an evaluation function, OpenMoveEval, which returns the number of moves that the AI minus the number of moves opponent can make, and your evaluation function performs correctly on some sample boards we provide. |\n",
    "| 30 points | Your AI defeats a random player >= 90% of the time. |\n",
    "| 20 points | Your AI defeats an agent with OpenMoveEval function that uses minimax to level 2  >= 65% of the times. |\n",
    "| 20 points | Your AI defeats an agent with OpenMoveEval function that uses alphabeta to level 4  >= 65% of the times. |\n",
    "| 20 points | Your AI defeats an agent with OpenMoveEval function that uses iterative deepening and alpha-beta pruning >= 65% of the time. |\n",
    "| 5 points | Your AI defeats an agent with Noah's secret evaluation function that uses iterative deepening and alpha-beta pruning and optimizes various aspects of the game player >= 85% of the time  |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Tips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hopefully, Assignment 0 got you pretty comfortable with Jupyter or at the very least addressed the major things that you may run into during this project. That said, Jupyter can take some getting used to, so here is a compilation of some things to watch out for specifically when it comes to Jupyter in a sort-of FAQs-like style"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. My Jupyter notebook does not seem to be starting up or my kernel is not starting correctly.**<br />\n",
    "Ans: This probably has to do with activating virtual environments. If you followed the setup instructions exactly, then you should activate your conda environment using `conda activate <environment_name>` from the Anaconda Prompt and start Jupyter Notebook from there.\n",
    "\n",
    "**2. I was running cell xxx when I opened up my notebook again and something or the other seems to have broken.**<br />\n",
    "Ans: This is one thing that is very different between IDEs like PyCharm and Jupyter Notebook. In Jupyter, every time you open a notebook, you should run all the cells that a cell depends on before running that cell. This goes for cells that are out of order too (if cell 5 depends on values set in cell 4 and 6, you need to run 4 and 6 before 5). Using the \"Run All\" command and its variants (found in the \"Cell\" dropdown menu above) should help you when you're in a situation like this.\n",
    "\n",
    "**3. The value of a variable in one of my cells is not what I expected it to be? What could have happened?** <br />\n",
    "Ans: You may have run a cell that modifies that variable too many times. Look at the \"counter\" example in assignment 0. First, try running `counter = 0` and then `counter += 1`. This way, when you print counter, you get counter = 1, right? Now try running `counter += 1` again, and now when you try to print the variable, you see a value of 2. This is similar to the issue from Question 2. The order in which you run the cells does affect the entire program, so be careful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have divided the tests into three sections. \n",
    "\n",
    "**assignment1: 1 submission per 30 minutes. This first section contains tests for the first two parts (OpenMoveEval, RandomPlayer).**\n",
    "\n",
    "**assignment1a: 1 submission per 120 minutes. This second section has tests for the middle two parts (Minimax, Alphabeta).**\n",
    "\n",
    "**assignment1b: 1 submission per 120 minutes. This third section has tests for last two parts (Iterative Deepening + AlphaBeta, +Noah's secret evaluation function).**\n",
    "\n",
    "These are split up into Section A (assignment1, assignment1a) and Section B (assignment1b) on Piazza for discussion.\n",
    "\n",
    "Submission policy: Grades will be based on the last submission made per section. (We are running our largest class to date, so we reserve the right to modify these rules depending upon the load on the servers)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Botfight (Extra Credit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to the basic assignment, you will have the option to compete against your peers for the glory of being the Fall 2018 AI-Game-Playing champ. We’ll set up a system to pit your AI against others, and we’ll be handing out extra credit for the top players. May the odds be ever in your favor.\n",
    "\n",
    "If you wish to compete in the tournament, simply include a plaintext file with a description of your agent, titled ‘AI.txt’, while submitting for the third section of tests (submit_b) and your CustomPlayer instance will be enlisted.\n",
    "\n",
    "If you compete in the AI tournament and your agent finishes in the top 10, you will receive bonus points for this assignment **(bonus points are added to the grades of each assignment. Not to final score. )**:\n",
    "\n",
    "- Best Overall:  12 bonus points added to the assignment score.\n",
    "- Second Best: 10 bonus points.\n",
    "- Third Best: 7 bonus points.\n",
    "- Fourth to Tenth Best: 5 bonus points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Submit Your Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A friendly reminder: please ensure that your submission is in `player_submission.ipynb`. The scripts described in the following section automatically send that file to the servers for processing.\n",
    "\n",
    "To submit your code and have it evaluated for a grade for first section, use `python submit.py`, for evaluation of second section use `python submit_a.py` and for third section use `python submit_b.py`.  Ensure that you have created the required AI.txt to enter the tournament.\n",
    "\n",
    "When you have finished implementing your code in `player_submission.ipynb`, run `notebook2script.py`. This script takes your notebook and produces a file called `submissions.py`, which is what the submit scripts will be looking for when sending your code to Gradescope. It only exports the cells that have `#export` at the top of them to a Python script so keep this in mind when producing `submissions.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These functions will inform the value judgements your AI will make when choosing moves. There are 2 classes:\n",
    "\n",
    "- `OpenMoveEvalFn` -Returns the number of available moves open for your player minus the number of moves available for opponent player. All baseline tests will use this function. **This is mandatory**\n",
    "- `CustomEvalFn` - You are encouraged to create your own evaluation function here.\n",
    "\n",
    "**DO** submit the code within this class (and only the code within this class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes on these classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. You may write additional code within each class. However, we will only be invoking the `score()` function. You may not change the signature of this function.\n",
    "2. When writing additional code to test, try to do so in separate classes (do not use ours). It allows for independent test execution and you can be sure that *all* the code within the EvalFn cells belong only to the EvalFn classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CustomPlayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the meat of the assignment. A few notes about the class:\n",
    "\n",
    "- You are not permitted to change the function signatures of any of the provided methods.\n",
    "- You are permitted to change the default values within the function signatures provided. In fact, when you have your custom evaluation function, you are encouraged to change the default values for `__init__` to use the new eval function.\n",
    "- You are free change the contents of each of the provided methods. When you are ready with `alphabeta()`, for example, you are encouraged to update `move()` to use that function instead.\n",
    "- You are free to add more methods to the class.\n",
    "- You may not create additional external functions and classes that are referenced from within this class.\n",
    "\n",
    "**DO** submit the code within this class (and only the code within this class)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In `player_submission_tests.ipynb` several built-in tests can be found in the **`main()`** function. We've included these to help you test your player and evaluation function as well as to give you an idea of how the classes are used. Feel free to play around with the code and add more tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Player classes (`test_players.py`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We include 2 player types for you to test against locally:\n",
    "\n",
    "- `RandomPlayer` - chooses a legal move randomly from among the available legal moves\n",
    "- `HumanPlayer` - allows *YOU* to play against the AI\n",
    "\n",
    "**DO NOT** submit these two players. You are however free to change these classes as you see fit. Know that any changes you make will be solely for the benefit of your own tests."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
